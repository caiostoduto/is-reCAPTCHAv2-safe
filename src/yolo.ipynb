{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acd5207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset AdityaJain1030/recaptcha-dataset already exists. Skipping download.\n",
      "Dataset nobodyPerfecZ/recaptchav2-29k already exists. Skipping download.\n",
      "Dataset cry2003/google-recaptcha-v2-images already exists. Skipping download.\n",
      "Dataset mikhailma/test-dataset already exists. Skipping download.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>From</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\datasets\\google-recaptcha-v2-images\\images\\...</td>\n",
       "      <td>00f4c717-7f51-4292-8d43-5cbe5cc1bc2d.jpg</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>google-recaptcha-v2-images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\datasets\\google-recaptcha-v2-images\\images\\...</td>\n",
       "      <td>0186fa9a-20bb-4dcf-8815-3528b0ee218d.jpg</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>google-recaptcha-v2-images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\datasets\\google-recaptcha-v2-images\\images\\...</td>\n",
       "      <td>01aa607e-b06a-41a6-bf86-b791704489f1.jpg</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>google-recaptcha-v2-images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\datasets\\google-recaptcha-v2-images\\images\\...</td>\n",
       "      <td>027d92ee-bd77-4e96-8063-e5ddd295fe4d.jpg</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>google-recaptcha-v2-images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\datasets\\google-recaptcha-v2-images\\images\\...</td>\n",
       "      <td>02b8d246-6447-4aaf-8cb8-e51e9ece56d2.jpg</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>google-recaptcha-v2-images</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image  \\\n",
       "0  ..\\datasets\\google-recaptcha-v2-images\\images\\...   \n",
       "1  ..\\datasets\\google-recaptcha-v2-images\\images\\...   \n",
       "2  ..\\datasets\\google-recaptcha-v2-images\\images\\...   \n",
       "3  ..\\datasets\\google-recaptcha-v2-images\\images\\...   \n",
       "4  ..\\datasets\\google-recaptcha-v2-images\\images\\...   \n",
       "\n",
       "                                   Filename    Label  \\\n",
       "0  00f4c717-7f51-4292-8d43-5cbe5cc1bc2d.jpg  Bicycle   \n",
       "1  0186fa9a-20bb-4dcf-8815-3528b0ee218d.jpg  Bicycle   \n",
       "2  01aa607e-b06a-41a6-bf86-b791704489f1.jpg  Bicycle   \n",
       "3  027d92ee-bd77-4e96-8063-e5ddd295fe4d.jpg  Bicycle   \n",
       "4  02b8d246-6447-4aaf-8cb8-e51e9ece56d2.jpg  Bicycle   \n",
       "\n",
       "                         From  \n",
       "0  google-recaptcha-v2-images  \n",
       "1  google-recaptcha-v2-images  \n",
       "2  google-recaptcha-v2-images  \n",
       "3  google-recaptcha-v2-images  \n",
       "4  google-recaptcha-v2-images  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "From\n",
       "google-recaptcha-v2-images    32265\n",
       "recaptchav2-29k               29568\n",
       "recaptcha-dataset             11774\n",
       "test-dataset                    279\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total images loaded: 73886'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.dataset_download import DatasetDownloader\n",
    "from utils.load_datasets import load_datasets\n",
    "\n",
    "dataset_downloader = DatasetDownloader(\"../datasets\")\n",
    "dataset_downloader.download_all()\n",
    "\n",
    "df = load_datasets(\"../datasets\")\n",
    "display(df.head())\n",
    "display(df['From'].value_counts())\n",
    "display(f\"Total images loaded: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shared images directory...\n",
      "Shared images directory created with 26773 images\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShared images directory created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(shared_images_dir.iterdir()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m df = pd.DataFrame(updt_df_dict)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m df.to_parquet(\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets_reduced.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Configuration\n",
    "max_images = 3000\n",
    "\n",
    "labels = df['Label'].unique()\n",
    "\n",
    "label_count = {label: 0 for label in labels}\n",
    "\n",
    "updt_df_dict = {\n",
    "    \"Image\": [],\n",
    "    \"Filename\": [],\n",
    "    \"Label\": [],\n",
    "    \"From\": []\n",
    "}\n",
    "\n",
    "# Create shared images directory once (outside the loop)\n",
    "shared_images_dir = Path(\"../yolo_dataset_shared/images\")\n",
    "shared_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create symlinks to all images once\n",
    "print(\"Creating shared images directory...\")\n",
    "for idx, row in df.iterrows():\n",
    "    if label_count[row['Label']] < max_images:\n",
    "        src = Path(row['Image']).absolute()\n",
    "        unique_filename = f\"{row['From']}_{row['Filename']}\"\n",
    "        dst = shared_images_dir / unique_filename\n",
    "        \n",
    "        if not dst.exists():\n",
    "            os.symlink(src, dst)\n",
    "        label_count[row['Label']] += 1\n",
    "\n",
    "        updt_df_dict[\"Image\"].append(row[\"Image\"])\n",
    "        updt_df_dict[\"Filename\"].append(row[\"Filename\"])\n",
    "        updt_df_dict[\"Label\"].append(row[\"Label\"])\n",
    "        updt_df_dict[\"From\"].append(row[\"From\"])\n",
    "\n",
    "print(f\"Shared images directory created with {len(list(shared_images_dir.iterdir()))} images\")\n",
    "\n",
    "df = pd.DataFrame(updt_df_dict)\n",
    "df.to_parquet(\"../datasets/datasets_reduced.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04d5e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Bicycle          3000\n",
       "Bus              3000\n",
       "Car              3000\n",
       "Hydrant          3000\n",
       "Crosswalk        3000\n",
       "Traffic Light    3000\n",
       "Other            3000\n",
       "Palm             2580\n",
       "Bridge           1831\n",
       "Stair             644\n",
       "Chimney           389\n",
       "Motorcycle        297\n",
       "Mountain           32\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    display(df['Label'].value_counts())\n",
    "except NameError:\n",
    "    df = pd.read_parquet('../datasets/datasets_reduced.parquet', engine='pyarrow')\n",
    "    display(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd7ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "Train: 21418, Val: 5355\n",
      "Train label distribution:\n",
      "Label\n",
      "Bicycle          2400\n",
      "Bus              2400\n",
      "Car              2400\n",
      "Hydrant          2400\n",
      "Crosswalk        2400\n",
      "Traffic Light    2400\n",
      "Other            2400\n",
      "Palm             2064\n",
      "Bridge           1464\n",
      "Stair             515\n",
      "Chimney           312\n",
      "Motorcycle        237\n",
      "Mountain           26\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      "Label\n",
      "Bicycle          600\n",
      "Bus              600\n",
      "Car              600\n",
      "Hydrant          600\n",
      "Crosswalk        600\n",
      "Traffic Light    600\n",
      "Other            600\n",
      "Palm             516\n",
      "Bridge           367\n",
      "Stair            129\n",
      "Chimney           77\n",
      "Motorcycle        60\n",
      "Mountain           6\n",
      "Name: count, dtype: int64\n",
      "Fold 1 dataset created!\n",
      "  - Labels file: ..\\yolo_dataset_fold0\\labels.txt\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "Train: 21418, Val: 5355\n",
      "Train label distribution:\n",
      "Label\n",
      "Bicycle          2400\n",
      "Bus              2400\n",
      "Car              2400\n",
      "Hydrant          2400\n",
      "Crosswalk        2400\n",
      "Traffic Light    2400\n",
      "Other            2400\n",
      "Palm             2064\n",
      "Bridge           1465\n",
      "Stair             515\n",
      "Chimney           311\n",
      "Motorcycle        237\n",
      "Mountain           26\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      "Label\n",
      "Bicycle          600\n",
      "Bus              600\n",
      "Car              600\n",
      "Hydrant          600\n",
      "Crosswalk        600\n",
      "Traffic Light    600\n",
      "Other            600\n",
      "Palm             516\n",
      "Bridge           366\n",
      "Stair            129\n",
      "Chimney           78\n",
      "Motorcycle        60\n",
      "Mountain           6\n",
      "Name: count, dtype: int64\n",
      "Fold 2 dataset created!\n",
      "  - Labels file: ..\\yolo_dataset_fold1\\labels.txt\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "Train: 21418, Val: 5355\n",
      "Train label distribution:\n",
      "Label\n",
      "Bicycle          2400\n",
      "Bus              2400\n",
      "Car              2400\n",
      "Hydrant          2400\n",
      "Crosswalk        2400\n",
      "Traffic Light    2400\n",
      "Other            2400\n",
      "Palm             2064\n",
      "Bridge           1465\n",
      "Stair             515\n",
      "Chimney           311\n",
      "Motorcycle        238\n",
      "Mountain           25\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      "Label\n",
      "Bicycle          600\n",
      "Bus              600\n",
      "Car              600\n",
      "Hydrant          600\n",
      "Crosswalk        600\n",
      "Traffic Light    600\n",
      "Other            600\n",
      "Palm             516\n",
      "Bridge           366\n",
      "Stair            129\n",
      "Chimney           78\n",
      "Motorcycle        59\n",
      "Mountain           7\n",
      "Name: count, dtype: int64\n",
      "Fold 3 dataset created!\n",
      "  - Labels file: ..\\yolo_dataset_fold2\\labels.txt\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "Train: 21419, Val: 5354\n",
      "Train label distribution:\n",
      "Label\n",
      "Bicycle          2400\n",
      "Bus              2400\n",
      "Car              2400\n",
      "Hydrant          2400\n",
      "Crosswalk        2400\n",
      "Traffic Light    2400\n",
      "Other            2400\n",
      "Palm             2064\n",
      "Bridge           1465\n",
      "Stair             516\n",
      "Chimney           311\n",
      "Motorcycle        238\n",
      "Mountain           25\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      "Label\n",
      "Bicycle          600\n",
      "Bus              600\n",
      "Car              600\n",
      "Hydrant          600\n",
      "Crosswalk        600\n",
      "Traffic Light    600\n",
      "Other            600\n",
      "Palm             516\n",
      "Bridge           366\n",
      "Stair            128\n",
      "Chimney           78\n",
      "Motorcycle        59\n",
      "Mountain           7\n",
      "Name: count, dtype: int64\n",
      "Fold 4 dataset created!\n",
      "  - Labels file: ..\\yolo_dataset_fold3\\labels.txt\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "Train: 21419, Val: 5354\n",
      "Train label distribution:\n",
      "Label\n",
      "Bicycle          2400\n",
      "Bus              2400\n",
      "Car              2400\n",
      "Hydrant          2400\n",
      "Crosswalk        2400\n",
      "Traffic Light    2400\n",
      "Other            2400\n",
      "Palm             2064\n",
      "Bridge           1465\n",
      "Stair             515\n",
      "Chimney           311\n",
      "Motorcycle        238\n",
      "Mountain           26\n",
      "Name: count, dtype: int64\n",
      "Val label distribution:\n",
      "Label\n",
      "Bicycle          600\n",
      "Bus              600\n",
      "Car              600\n",
      "Hydrant          600\n",
      "Crosswalk        600\n",
      "Traffic Light    600\n",
      "Other            600\n",
      "Palm             516\n",
      "Bridge           366\n",
      "Stair            129\n",
      "Chimney           78\n",
      "Motorcycle        59\n",
      "Mountain           6\n",
      "Name: count, dtype: int64\n",
      "Fold 5 dataset created!\n",
      "  - Labels file: ..\\yolo_dataset_fold4\\labels.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "shared_images_dir = Path(\"../yolo_dataset_shared/images\")\n",
    "\n",
    "N_FOLDS = 5\n",
    "# Setup k-fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "labels = df['Label'].unique()\n",
    "\n",
    "# Store results from all folds\n",
    "all_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['Label'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "    print(f\"Train label distribution:\\n{train_df['Label'].value_counts()}\")\n",
    "    print(f\"Val label distribution:\\n{val_df['Label'].value_counts()}\")\n",
    "    \n",
    "    # Create fold-specific dataset structure\n",
    "    dataset_root = Path(f\"../yolo_dataset_fold{fold}\")\n",
    "    labels_file = dataset_root / \"labels.txt\"\n",
    "    \n",
    "    dataset_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Build list of images with their splits for this fold\n",
    "    all_images = []\n",
    "    \n",
    "    # Add training images\n",
    "    for idx, row in train_df.iterrows():\n",
    "        unique_filename = f\"{row['From']}_{row['Filename']}\"\n",
    "        all_images.append({\n",
    "            'filename': unique_filename,\n",
    "            'label': row['Label'],\n",
    "            'split': 'train'\n",
    "        })\n",
    "    \n",
    "    # Add validation images\n",
    "    for idx, row in val_df.iterrows():\n",
    "        unique_filename = f\"{row['From']}_{row['Filename']}\"\n",
    "        all_images.append({\n",
    "            'filename': unique_filename,\n",
    "            'label': row['Label'],\n",
    "            'split': 'val'\n",
    "        })\n",
    "    \n",
    "    # Create labels file with format: filename label split\n",
    "    with open(labels_file, 'w') as f:\n",
    "        f.write(\"filename\\tlabel\\tsplit\\n\")\n",
    "        for img_info in all_images:\n",
    "            f.write(f\"{img_info['filename']}\\t{img_info['label']}\\t{img_info['split']}\\n\")\n",
    "    \n",
    "    # For YOLO classification, create train/val split directories\n",
    "    # with symlinks pointing to the shared images folder\n",
    "    train_dir = dataset_root / \"train\"\n",
    "    val_dir = dataset_root / \"val\"\n",
    "    \n",
    "    # Remove old train/val if they exist\n",
    "    if train_dir.exists():\n",
    "        shutil.rmtree(train_dir)\n",
    "    if val_dir.exists():\n",
    "        shutil.rmtree(val_dir)\n",
    "    \n",
    "    # Create label subdirectories\n",
    "    for label in labels:\n",
    "        (train_dir / label).mkdir(parents=True, exist_ok=True)\n",
    "        (val_dir / label).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create symlinks in train/val pointing to shared images folder\n",
    "    for img_info in all_images:\n",
    "        src = (shared_images_dir / img_info['filename']).absolute()\n",
    "        if img_info['split'] == 'train':\n",
    "            dst = train_dir / img_info['label'] / img_info['filename']\n",
    "        else:\n",
    "            dst = val_dir / img_info['label'] / img_info['filename']\n",
    "        \n",
    "        os.symlink(src, dst)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} dataset created!\")\n",
    "    print(f\"  - Labels file: {labels_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=10, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=..\\yolo_dataset_fold0, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold0_yolo1110, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=is_recaptchav2_safe_kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\train... found 21418 images in 13 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\val... found 5355 images in 13 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    346893  ultralytics.nn.modules.head.Classify         [256, 13]                     \n",
      "YOLO11n-cls summary: 86 layers, 1,547,757 parameters, 1,547,757 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 0.10.0 MB/s, size: 5.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\train... 21418 images, 0 corrupt: 100% ━━━━━━━━━━━━ 21418/21418 594.3it/s 36.0s<0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 0.50.2 MB/s, size: 6.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\val... 5355 images, 0 corrupt: 100% ━━━━━━━━━━━━ 5355/5355 310.9it/s 17.2s<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.00046875), 40 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mE:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K        1/2      0.24G      1.178          8        224: 100% ━━━━━━━━━━━━ 2142/2142 3.6it/s 9:499<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 2.6it/s 1:43<0.3ss\n",
      "                   all      0.855      0.992\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K        2/2     0.248G     0.6524          8        224: 100% ━━━━━━━━━━━━ 2142/2142 29.3it/s 1:13<0.0ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 21.1it/s 12.7s0.2s\n",
      "                   all      0.876      0.996\n",
      "\n",
      "2 epochs completed in 0.218 hours.\n",
      "Optimizer stripped from E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110\\weights\\last.pt, 3.2MB\n",
      "Optimizer stripped from E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110\\weights\\best.pt, 3.2MB\n",
      "\n",
      "Validating E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110\\weights\\best.pt...\n",
      "Ultralytics 8.3.228  Python-3.13.7 torch-2.9.1+cu126 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,542,677 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\train... found 21418 images in 13 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold0\\val... found 5355 images in 13 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 92.5it/s 2.9s<0.1s\n",
      "                   all      0.876      0.996\n",
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold0_yolo1110\u001b[0m\n",
      "\n",
      "Fold 1 completed!\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=10, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=..\\yolo_dataset_fold1, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold1_yolo11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=is_recaptchav2_safe_kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\train... found 21418 images in 13 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\val... found 5355 images in 13 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    346893  ultralytics.nn.modules.head.Classify         [256, 13]                     \n",
      "YOLO11n-cls summary: 86 layers, 1,547,757 parameters, 1,547,757 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 35.812.3 MB/s, size: 5.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\train... 21418 images, 0 corrupt: 100% ━━━━━━━━━━━━ 21418/21418 5.6Kit/s 3.8s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 0.60.2 MB/s, size: 5.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\val... 5355 images, 0 corrupt: 100% ━━━━━━━━━━━━ 5355/5355 334.7it/s 16.0s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.00046875), 40 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mE:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K        1/2     0.258G      1.182          8        224: 100% ━━━━━━━━━━━━ 2142/2142 3.7it/s 9:388<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 2.7it/s 1:388<0.1s\n",
      "                   all      0.855      0.993\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K        2/2     0.268G     0.6508          8        224: 100% ━━━━━━━━━━━━ 2142/2142 30.4it/s 1:10<0.1ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 77.6it/s 3.5s0.1s\n",
      "                   all      0.877      0.994\n",
      "\n",
      "2 epochs completed in 0.209 hours.\n",
      "Optimizer stripped from E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11\\weights\\last.pt, 3.2MB\n",
      "Optimizer stripped from E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11\\weights\\best.pt, 3.2MB\n",
      "\n",
      "Validating E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11\\weights\\best.pt...\n",
      "Ultralytics 8.3.228  Python-3.13.7 torch-2.9.1+cu126 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,542,677 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\train... found 21418 images in 13 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold1\\val... found 5355 images in 13 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 268/268 99.8it/s 2.7s0.1ss\n",
      "                   all      0.876      0.994\n",
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold1_yolo11\u001b[0m\n",
      "\n",
      "Fold 2 completed!\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=10, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=..\\yolo_dataset_fold2, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fold2_yolo11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=is_recaptchav2_safe_kfold, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold2_yolo11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\train... found 21418 images in 13 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\val... found 5355 images in 13 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    346893  ultralytics.nn.modules.head.Classify         [256, 13]                     \n",
      "YOLO11n-cls summary: 86 layers, 1,547,757 parameters, 1,547,757 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 38.911.0 MB/s, size: 5.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\train... 21418 images, 0 corrupt: 100% ━━━━━━━━━━━━ 21418/21418 5.5Kit/s 3.9s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 42.12.7 MB/s, size: 5.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\val... 5355 images, 0 corrupt: 100% ━━━━━━━━━━━━ 5355/5355 5.7Kit/s 0.9s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\yolo_dataset_fold2\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.00046875), 40 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mE:\\Backups\\Estudos\\Faculdade\\IA\\is-reCAPTCHAv2-safe\\src\\is_recaptchav2_safe_kfold\\fold2_yolo11\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K        1/2     0.234G      2.812         10        224: 0% ──────────── 1/2142 1.1s/it 0.3s<38:30\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from ultralytics import YOLO\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "classification_models = [\"yolo11n-cls.pt\", \"yolo11s-cls.pt\", \"yolo11m-cls.pt\", \"yolo11l-cls.pt\", \"yolo11x-cls.pt\"]\n",
    "model_name = classification_models[0]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Store results from all folds\n",
    "all_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['Label'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Create fold-specific dataset structure\n",
    "    dataset_root = Path(f\"../yolo_dataset_fold{fold}\")\n",
    "    labels_file = dataset_root / \"labels.txt\"\n",
    "    \n",
    "    # Initialize fresh model for each fold\n",
    "    model = YOLO(\"yolo11n-cls.pt\")\n",
    "    model.to('cuda')\n",
    "    # Train on this fold\n",
    "    results = model.train(\n",
    "        data=str(dataset_root),\n",
    "        epochs=2,\n",
    "        imgsz=224,\n",
    "        batch=10,\n",
    "        patience=50,\n",
    "        save=True,\n",
    "        project=\"is_recaptchav2_safe_kfold\",\n",
    "        name=f\"fold{fold}_yolo11\",\n",
    "        plots=True,\n",
    "        val=True,\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    all_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'results': results,\n",
    "        'final_metrics': results.results_dict if hasattr(results, 'results_dict') else None\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} completed!\")\n",
    "\n",
    "# Summary of all folds\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Extract and display metrics across folds\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Fold': r['fold'],\n",
    "        # Add relevant metrics from results here\n",
    "        # e.g., 'Accuracy': r['results'].top1 if hasattr(r['results'], 'top1') else None\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Calculate average performance\n",
    "print(\"\\nAverage Performance Across Folds:\")\n",
    "for col in metrics_df.columns:\n",
    "    if col != 'Fold' and pd.api.types.is_numeric_dtype(metrics_df[col]):\n",
    "        mean_val = metrics_df[col].mean()\n",
    "        std_val = metrics_df[col].std()\n",
    "        print(f\"{col}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"\\nAll folds completed! Check 'is_recaptchav2_safe_kfold/' for individual fold results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is-recaptchav2-safe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
